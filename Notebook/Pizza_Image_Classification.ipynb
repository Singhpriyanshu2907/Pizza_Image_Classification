{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7cyGKdNLPRri"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.regularizers import l2\n",
        "from PIL import UnidentifiedImageError\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "import random\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9dOFBCah-Gm",
        "outputId": "25779404-787b-4393-8dc3-bed9669d74b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TF-sPKBPzsO",
        "outputId": "e17643ff-9966-4eaa-f54d-39bad7e05a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "patVuMNoQRjF",
        "outputId": "6d42d01d-c97f-46ac-d5dd-f5f435c38baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJUlFmKMP0T1"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/drive/MyDrive/Pizza Image Classification/Data.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/Pizza Image Classification\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eYjDB-ByQN7n"
      },
      "outputs": [],
      "source": [
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QqiPyU_gX5ji"
      },
      "outputs": [],
      "source": [
        "# Path to your extracted dataset in Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/Pizza Image Classification/Data'\n",
        "\n",
        "# Define the class folders\n",
        "pizza_folder = os.path.join(dataset_path, 'pizza')\n",
        "not_pizza_folder = os.path.join(dataset_path, 'not_pizza')\n",
        "\n",
        "# Define destination paths for training and validation sets\n",
        "data_path = '/content/drive/MyDrive/Pizza Image Classification'\n",
        "train_dir = os.path.join(data_path, 'train')\n",
        "val_dir = os.path.join(data_path, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yr-ljT2sQJlT"
      },
      "outputs": [],
      "source": [
        "# Create directories if they do not exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'Pizza'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'Not pizza'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'Pizza'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'Not pizza'), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AddOcSwWP0Xe",
        "outputId": "e0719720-7f16-4ab3-8140-2d07659b8954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully split into training and validation sets!\n"
          ]
        }
      ],
      "source": [
        "# Helper function to split data\n",
        "def split_data(source_dir, train_dir, val_dir, class_name, test_size=0.2):\n",
        "    # List all files in the source directory\n",
        "    all_files = os.listdir(source_dir)\n",
        "\n",
        "    # Get file paths\n",
        "    file_paths = [os.path.join(source_dir, f) for f in all_files if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_files, val_files = train_test_split(file_paths, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Copy files to the respective directories\n",
        "    for file_path in train_files:\n",
        "        shutil.copy(file_path, os.path.join(train_dir, class_name))\n",
        "\n",
        "    for file_path in val_files:\n",
        "        shutil.copy(file_path, os.path.join(val_dir, class_name))\n",
        "\n",
        "# Split data for each class\n",
        "split_data(pizza_folder, train_dir, val_dir, 'Pizza')\n",
        "split_data(not_pizza_folder, train_dir, val_dir, 'Not pizza')\n",
        "\n",
        "print('Data has been successfully split into training and validation sets!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li8IlxDeP0aM",
        "outputId": "9045a170-95cb-40a8-afc8-380b34be80d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set - Pizza: 400 images\n",
            "Training set - Not pizza: 399 images\n",
            "Validation set - Pizza: 100 images\n",
            "Validation set - Not pizza: 100 images\n"
          ]
        }
      ],
      "source": [
        "# Function to count files in each directory\n",
        "def count_files(directory):\n",
        "    count = 0\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        count += len(files)\n",
        "    return count\n",
        "\n",
        "# Print counts for each set\n",
        "print(f\"Training set - Pizza: {count_files(os.path.join(train_dir, 'Pizza'))} images\")\n",
        "print(f\"Training set - Not pizza: {count_files(os.path.join(train_dir, 'Not pizza'))} images\")\n",
        "print(f\"Validation set - Pizza: {count_files(os.path.join(val_dir, 'Pizza'))} images\")\n",
        "print(f\"Validation set - Not pizza: {count_files(os.path.join(val_dir, 'Not pizza'))} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzz-QrkYP0dF"
      },
      "outputs": [],
      "source": [
        "def display_random_images(image_dir, class_name, num_images=5):\n",
        "    \"\"\"\n",
        "    Display random images from the specified directory.\n",
        "\n",
        "    Args:\n",
        "        image_dir (str): Path to the directory containing images.\n",
        "        class_name (str): The class name (Pizza or Not pizza).\n",
        "        num_images (int): Number of random images to display.\n",
        "    \"\"\"\n",
        "    # Get list of all images in the directory\n",
        "    all_images = os.listdir(image_dir)\n",
        "\n",
        "    # Select a random subset of images\n",
        "    random_images = random.sample(all_images, min(len(all_images), num_images))\n",
        "\n",
        "    # Plot the random images\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i, img_name in enumerate(random_images):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        img = mpimg.imread(img_path)\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{class_name} - {img_name}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afY5TTKCP0ft"
      },
      "outputs": [],
      "source": [
        "# Paths to the training and validation directories\n",
        "train_pizza_dir = '/content/drive/MyDrive/Pizza Image Classification/train/Pizza'\n",
        "train_not_pizza_dir = '/content/drive/MyDrive/Pizza Image Classification/train/Not pizza'\n",
        "val_pizza_dir = '/content/drive/MyDrive/Pizza Image Classification/val/Pizza'\n",
        "val_not_pizza_dir = '/content/drive/MyDrive/Pizza Image Classification/val/Not pizza'\n",
        "\n",
        "# Display random images from each set\n",
        "print(\"Training Set - Pizza\")\n",
        "display_random_images(train_pizza_dir, 'Pizza', num_images=3)\n",
        "\n",
        "print(\"Training Set - Not pizza\")\n",
        "display_random_images(train_not_pizza_dir, 'Not pizza', num_images=3)\n",
        "\n",
        "print(\"Validation Set - Pizza\")\n",
        "display_random_images(val_pizza_dir, 'Pizza', num_images=3)\n",
        "\n",
        "print(\"Validation Set - Not pizza\")\n",
        "display_random_images(val_not_pizza_dir, 'Not pizza', num_images=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebRqchVtdQbU"
      },
      "outputs": [],
      "source": [
        "# To check if the images are in the correct fromat.\n",
        "def check_images(directory):\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            try:\n",
        "                img_path = os.path.join(root, file)\n",
        "                img = Image.open(img_path)\n",
        "                img.verify()  # Check if image can be opened\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted or invalid image file: {img_path}\")\n",
        "\n",
        "# Check both the training and validation directories\n",
        "check_images('/content/drive/MyDrive/Pizza Image Classification/train')\n",
        "check_images('/content/drive/MyDrive/Pizza Image Classification/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjNcGAGSU8In",
        "outputId": "f05d67ba-ee10-4814-8d47-9c9c806d9421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 799 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the images from the training directory\n",
        "train_dir = '/content/drive/MyDrive/Pizza Image Classification/train'\n",
        "val_dir = '/content/drive/MyDrive/Pizza Image Classification/val'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkArZ31JX2xW"
      },
      "outputs": [],
      "source": [
        "# Load the VGG16 model with pretrained weights and exclude the top classification layers\n",
        "vgg16 = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the layers of the VGG16 model\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define input tensor\n",
        "inputs = vgg16.input\n",
        "\n",
        "# Add Flatten layer to convert 2D outputs to 1D\n",
        "x = Flatten()(vgg16.output)\n",
        "\n",
        "# Add Dense layer with L2 regularization and ReLU activation\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Add Dropout layer to prevent overfitting\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add output Dense layer for binary classification with softmax\n",
        "outputs = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary to check the architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Jn_Z3McP0k0"
      },
      "outputs": [],
      "source": [
        "# Calculate steps per epoch\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = val_generator.samples // val_generator.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Mdul4xeX0J"
      },
      "outputs": [],
      "source": [
        "# Add EarlyStopping to stop training when the validation loss does not improve\n",
        "earlystopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# fit the model\n",
        "r = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[earlystopping]  # Include early stopping callback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlIU5Eu4P0nM"
      },
      "outputs": [],
      "source": [
        "## MODEL 2 VGG19\n",
        "\n",
        "# Load the VGG19 model with pretrained weights and exclude the top classification layers\n",
        "vgg19 = VGG19(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the layers of the VGG19 model\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define input tensor\n",
        "inputs = vgg19.input\n",
        "\n",
        "# Add Flatten layer to convert 2D outputs to 1D\n",
        "x = Flatten()(vgg19.output)\n",
        "\n",
        "# Add Dense layer with L2 regularization and ReLU activation\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Add Dropout layer to prevent overfitting\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add output Dense layer for binary classification with softmax\n",
        "outputs = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Create the model\n",
        "model2 = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical cross-entropy loss\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary to check the architecture\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8m2HRCAU92sz"
      },
      "outputs": [],
      "source": [
        "# Set early stopping callback to monitor validation loss\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',        # Monitor the validation loss\n",
        "    patience=3,                # Stop training after 3 epochs without improvement\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tylwVboEP0pk"
      },
      "outputs": [],
      "source": [
        "history = model2.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=30,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping]  # Include early stopping callback\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model2.evaluate(val_generator)\n",
        "print(f'Validation Loss: {loss}')\n",
        "print(f'Validation Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model RESNET50\n",
        "\n",
        "# Define the ResNet model architecture in a function\n",
        "resnet = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the layers of ResNet\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "inputs = resnet.input\n",
        "x = Flatten()(resnet.output)\n",
        "\n",
        "# Add dense layer with regularization and dropout\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "outputs = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Create the model\n",
        "model3 = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary to check the architecture\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "TWWLAshq6JU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phm_z5-sP0zm"
      },
      "outputs": [],
      "source": [
        "# Define EarlyStopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Fit the model\n",
        "history = model3.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stop],  # Add the early stopping callback here\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpIozha8P01-"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "model_dir = '/content/drive/MyDrive/Pizza Image Classification/Model'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the model in the \"Model\" folder\n",
        "model2.save(os.path.join(model_dir, 'vgg19_model.h5'))\n",
        "\n",
        "print(f\"Model saved at {os.path.join(model_dir, 'vgg19_model.h5')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "c91cb0BxP04M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308ef74b-2d5a-4100-bde4-1d3119ec8265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model from the directory\n",
        "model_path = '/content/drive/MyDrive/Pizza Image Classification/Model/vgg19_model.h5'\n",
        "Classifier = load_model(model_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your folder with images\n",
        "image_folder = '/content/drive/MyDrive/Pizza Image Classification/Modeltrial'\n",
        "\n",
        "\n",
        "# Map class indices to labels (update based on your actual classes)\n",
        "class_labels = {1: 'Pizza', 0: 'Not Pizza'}\n",
        "\n",
        "# Iterate over every image in the folder\n",
        "for img_file in os.listdir(image_folder):\n",
        "    # Construct the full image path\n",
        "    img_path = os.path.join(image_folder, img_file)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Resize the image to 224x224\n",
        "    img_array = image.img_to_array(img)  # Convert the image to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension (1, 224, 224, 3)\n",
        "    img_array /= 255.0  # Normalize the image\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = Classifier.predict(img_array)\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Print the result for each image\n",
        "    print(f\"Image: {img_file} -> Predicted class: {class_labels[predicted_class[0]]}\")\n"
      ],
      "metadata": {
        "id": "n948Z72c3tgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}